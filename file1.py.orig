# nlp_processing.py
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk import ne_chunk, pos_tag
from nltk.sentiment import SentimentIntensityAnalyzer
from collections import defaultdict, Counter
from itertools import islice
import matplotlib.pyplot as plt
from wordcloud import WordCloud




<<<<<<< HEAD
def pos_tagging(self):
        return pos_tag(self.tokens)

    def sentence_tokenize(self):
        return self.sentences

    def named_entity_recognition(self):
        return ne_chunk(pos_tag(self.tokens))

    def word_frequency(self):
        filtered = self.remove_stopwords()
        return Counter(filtered).most_common(10)

    def generate_ngrams(self, n=2):
        tokens = self.remove_stopwords()
        ngrams = zip(*[tokens[i:] for i in range(n)])
        return list(ngrams)[:10]

    def sentiment_analysis(self):
        return self.sia.polarity_scores(self.text)
    def generate_wordcloud(self):
        filtered = ' '.join(self.remove_stopwords())
        wc = WordCloud(width=800, height=400, background_color='white').generate(filtered)
        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title("Word Cloud")
        plt.show()

    def analyze_all(self):
        print("\n=== NLP PROCESSING ===")
        print("Original Tokens:", self.tokens[:10])
        print("Stopwords Removed:", self.remove_stopwords()[:10])
        print("Stemmed Words:", self.stem_words()[:10])
        print("Lemmatized Words:", self.lemmatize_words()[:10])
        print("POS Tags:", self.pos_tagging()[:10])
        print("Sentences:", self.sentence_tokenize()[:2])
        print("NER:", self.named_entity_recognition())
        print("Top Word Frequencies:", self.word_frequency())
        print("Bigrams:", self.generate_ngrams(2))
        print("Sentiment:", self.sentiment_analysis())
        self.generate_wordcloud()

def run_nlp_demo():
=======
# ai_features.py
from textblob import TextBlob
from gensim.summarization import keywords, summarize
import spacy
from collections import Counter
import os

# Load spaCy model safely
try:
    nlp = spacy.load("en_core_web_sm")
except:
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

class AIAnalyzer:
    def _init_(self, text):
        self.text = text
        self.blob = TextBlob(text)
        self.doc = nlp(text)

    def get_sentiment(self):
        polarity = self.blob.sentiment.polarity
        subjectivity = self.blob.sentiment.subjectivity
        return {"polarity": polarity, "subjectivity": subjectivity}

    def get_sentence_level_sentiment(self):
        return [{"sentence": str(sent), "sentiment": TextBlob(str(sent)).sentiment.polarity}
                for sent in self.blob.sentences]

    def get_keywords(self):
        try:
            return keywords(self.text, ratio=0.1, words=10).split('\n')
        except:
            return ["Not enough data to extract keywords."]


   def get_summary(self):
        try:
            return summarize(self.text, ratio=0.3)
        except:
            return "Summary not available due to short input."

    def named_entities(self):
        return [(ent.text, ent.label_) for ent in self.doc.ents]

    def detect_language(self):
        try:
            return self.blob.detect_language()
        except:
            return "Language detection failed."

        def extract_topics(self):
        # Simple noun chunk frequency
        noun_chunks = [chunk.text.lower() for chunk in self.doc.noun_chunks]
        top_chunks = Counter(noun_chunks).most_common(5)
        return top_chunks

    def pos_statistics(self):
        pos_counts = Counter([token.pos_ for token in self.doc])
        return dict(pos_counts)

    def analyze(self):
        print("\n=== AI ANALYSIS ===")
        print("Language Detected:", self.detect_language())
        print("Sentiment:", self.get_sentiment())
        print("\nSentence-Level Sentiment:")
        for item in self.get_sentence_level_sentiment()[:3]:
            print(f"  - {item['sentence']} => Sentiment: {item['sentiment']:.2f}")


print("\nTop Keywords:", self.get_keywords())
        print("\nNamed Entities:", self.named_entities())
        print("\nTopics Extracted (Top Noun Phrases):", self.extract_topics())
        print("\nPOS Tag Statistics:", self.pos_statistics())
        print("\nSummary:\n", self.get_summary())

def ai_demo():
>>>>>>> c
    from text_input import TextLoader
    loader = TextLoader("sample.txt")
    loader.read_file()
    loader.clean_text()

<<<<<<< HEAD
    processor = NLPProcessor(loader.cleaned_text)
    processor.analyze_all()

if _name_ == "_main_":
    run_nlp_demo()
=======
    analyzer = AIAnalyzer(loader.cleaned_text)
    analyzer.analyze()

if _name_ == "_main_":
    ai_demo()






>>>>>>> c
